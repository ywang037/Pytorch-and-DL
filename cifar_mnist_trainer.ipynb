{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a51bee",
   "metadata": {},
   "source": [
    "# About\n",
    "This is the step-by-step version of `./src/mybaseline-all-in-one.py`\n",
    "\n",
    "### Model hyper-parameters\n",
    "1. Model `CNNMnistWyBn` adds batch normalization `nn.BatchNorm2d` after convolutional filter `nn.Conv2d`, which leads to better performance in baseline learning of MNIST. Especially when learning rate is larger, 0.1. \n",
    "2. Adding dropout layer `nn.Dropout2d` after the second convolutional filter has better effects on improvement of test accuracy than addind dropout after the first convolutional filter or after the fc layer. However, in the baseline learning of MNIST, adding batch normalization outperform adding dropout layer.\n",
    "3. In addition to the batch normalization, adding a dropout layer before the second convoluation filter boosts performance even further, having obvious effect on preventing overfitting, which is more effective for smaller learning rate.\n",
    "    * However, in a test run under non-IID case {E=5, B=10, C=0.0} the validation performance is lower than original model `CNNMnistWy` which does not have batch normalization nor dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beea3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ccd31",
   "metadata": {},
   "source": [
    "##### Use tensorboard writer to visualize the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d78380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa4574b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wangyuan\\\\my-torch-handson\\\\Pytorch-and-DL'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a4f1fe",
   "metadata": {},
   "source": [
    "#### Option classes and help function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "454f6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskMnist():\n",
    "    def __init__(self, nn='cnn'):\n",
    "        self.path = '..\\data\\mnist'\n",
    "        self.name = 'mnist'\n",
    "        self.nn = nn\n",
    "        \n",
    "class TaskCifar():\n",
    "    def __init__(self,nn='torch'):\n",
    "        self.path = '..\\data\\cifar'\n",
    "        self.name = 'cifar'\n",
    "        self.nn = nn\n",
    "\n",
    "class HyperParam():\n",
    "    def __init__(self,path,learning_rate=0.1, batch_size=100, epoch=10, momentum=0.9, nesterov=False):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.datapath = path\n",
    "        self.lr=learning_rate\n",
    "        self.bs=batch_size\n",
    "        self.epoch=epoch\n",
    "        self.momentum=momentum\n",
    "        self.nesterov=nesterov        \n",
    "    \n",
    "# the function used to count the number of trainable parameters\n",
    "def get_count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280895d3",
   "metadata": {},
   "source": [
    "#### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d23bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 2NN model described in the vanilla FL paper for experiments with MNIST\n",
    "class TwoNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoNN,self).__init__()\n",
    "        self.nn_layer=nn.Sequential(\n",
    "            nn.Linear(in_features=28*28,out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100,out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100,out_features=10)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,28*28)\n",
    "        logits = self.nn_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "                 \n",
    "# the 2NN model in AshwinRJ's repository\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_input = nn.Linear(28*28, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.layer_hidden = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1]*x.shape[-2]*x.shape[-1])\n",
    "        x = self.layer_input(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_hidden(x)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "\n",
    "# the CNN model describted in the vanilla FL paper for experiments with MNIST\n",
    "class CNNMnistWy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnistWy,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512,out_features=10),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1,1024)\n",
    "        logits = self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "\n",
    "# Same with CNNMnistWy with batchnormalization\n",
    "class CNNMnistWyBn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnistWyBn,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512,out_features=10),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1,1024)\n",
    "        logits = self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "    \n",
    "# Same with CNNMnistWyBn with additional dropout layer after conv filter\n",
    "class CNNMnistWyBnDp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnistWyBnDp,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512,out_features=10),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1,1024)\n",
    "        logits = self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "\n",
    "# the CNN model in AshwinRJ's repository\n",
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# CNN created by WY following description in vanilla FL paper\n",
    "class CNNCifarWy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNCifarWy,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256,out_features=10),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1,1024)\n",
    "        logits = self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)    \n",
    "    \n",
    "# the example model used in the official CNN training tutorial of PyTorch using CIFAR10\n",
    "# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "class CNNCifarTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNCifarTorch,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "#             nn.Dropout2d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(16*5*5,120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120,84),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(),\n",
    "            nn.Linear(84,10)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1, 16 * 5 * 5)\n",
    "        logits=self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "\n",
    "# the exmaple model used in the official CNN tutorial of TensorFlow using CIFAR10\n",
    "# https://www.tensorflow.org/tutorials/images/cnn\n",
    "class CNNCifarTf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNCifarTf,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=32, kernel_size=3), # output size 30*30, i.e., (32, 30 ,30)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # output size 15*15, i.e., (32, 15 ,15)\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3), # output size 13*13, i.e., (64, 13 ,13)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # output size 6*6, i.e., (64, 6, 6)\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3), # output size 4*4, i.e., (64, 4, 4)\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64,out_features=10),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1,1024)\n",
    "        logits=self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "\n",
    "# WY's edition of cnn models from TF, adding a dropout layer\n",
    "class CNNCifarTfDp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNCifarTfDp,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=32, kernel_size=3), # output size 30*30, i.e., (32, 30 ,30)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # output size 15*15, i.e., (32, 15 ,15)\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3), # output size 13*13, i.e., (64, 13 ,13)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # output size 6*6, i.e., (64, 6, 6)\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3), # output size 4*4, i.e., (64, 4, 4)\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64,out_features=10),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1,1024)\n",
    "        logits=self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274c3f9",
   "metadata": {},
   "source": [
    "#### Define data loader functions\n",
    "**BE CAUTION** that, the transform applied to the test data loader should be **the same as the training data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81d73b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cifar(path, batch_size=100):\n",
    "    \"\"\"\n",
    "    returns training data loader and test data loader\n",
    "    \"\"\"\n",
    "    # no brainer normalization used in the pytorch tutorial\n",
    "    mean_0 = (0.5, 0.5, 0.5)\n",
    "    std_0 = (0.5, 0.5, 0.5)\n",
    "\n",
    "    # alternative normilzation\n",
    "    mean_1 = (0.4914, 0.4822, 0.4465)\n",
    "    std_1 = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "    # configure tranform for training data\n",
    "    # standard transform used in the pytorch tutorial \n",
    "    transform_train_0 = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_0,std_0),\n",
    "    ])\n",
    "\n",
    "    # configure transform for test data\n",
    "    transform_test_0 = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_0,std_0),\n",
    "    ])\n",
    "    \n",
    "    '''\n",
    "    # Alternative transform\n",
    "    # enhanced transform, random crop and flip is optional\n",
    "    transform_train_1 = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_1,std_1),\n",
    "    ])\n",
    "\n",
    "    # alternative, only random crop is used\n",
    "    transform_train_2 = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_1,std_1),\n",
    "    ])\n",
    "\n",
    "    # configure transform for test data\n",
    "    transform_test_1 = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_1,std_1),\n",
    "    ])\n",
    "    '''\n",
    "    # setup the CIFAR10 training dataset\n",
    "    data_train = datasets.CIFAR10(root=path, train=True, download=False, transform=transform_train_0)\n",
    "    loader_train = data.DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # setup the CIFAR10 test dataset\n",
    "    data_test = datasets.CIFAR10(root=path, train=False, download=False, transform=transform_test_0)\n",
    "    loader_test = data.DataLoader(data_test, batch_size=100, shuffle=False)\n",
    "\n",
    "    return loader_train, loader_test\n",
    "\n",
    "def data_mnist(path,batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # setup the MNIST training dataset\n",
    "    data_train = datasets.MNIST(root=path, train=True, download=False, transform=transform)\n",
    "    loader_train = data.DataLoader(data_train, batch_size=batch_size, shuffle=True) \n",
    "    \n",
    "    # setup the MNIST training dataset\n",
    "    data_test = datasets.MNIST(root=path, train=False, download=False, transform=transform)\n",
    "    loader_test = data.DataLoader(data_test, batch_size=100, shuffle=False)\n",
    "    return loader_train,loader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca404f4a",
   "metadata": {},
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac928255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(loader_train, loader_test, epochs, loss_fn, optimizer, decay, device):\n",
    "    # set a learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer,gamma=decay,verbose=True)\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "        test_acc = 0.0\n",
    "\n",
    "        # training of each epoch\n",
    "        model.train()\n",
    "        for batch, (images, labels) in enumerate(loader_train):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        train_loss /= len(loader_train.dataset)\n",
    "\n",
    "        # test after each epoch\n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        with torch.no_grad():\n",
    "            for batch, (images, labels) in enumerate(loader_test):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs,labels)\n",
    "                test_loss += loss.item() * images.size(0)\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                num_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        test_loss /= len(loader_test.dataset)\n",
    "        test_acc = num_correct/len(loader_test.dataset)\n",
    "        print('Epoch: {} | Training Loss: {:.2f} | Test Loss: {:.2f} | Test accuracy = {:.2f}%'.format(epoch, train_loss, test_loss, 100*test_acc))\n",
    "\n",
    "        # update the learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # user tensorboard writer\n",
    "        writer.add_scalar(\"Train loss\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Test loss\", test_loss, epoch)\n",
    "        writer.add_scalar(\"Test acc\", test_acc, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4623c2d",
   "metadata": {},
   "source": [
    "#### Setup the training task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b2fa554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tfdp with cifar\n"
     ]
    }
   ],
   "source": [
    "# configure the task\n",
    "# task = TaskMnist(nn='cnn_wy_bndp')\n",
    "task = TaskCifar(nn='tfdp')\n",
    "\n",
    "# configure the training parameters\n",
    "settings = HyperParam(path=task.path, learning_rate=0.1, nesterov=False, batch_size=100, epoch=100)\n",
    "print('Train', task.nn, 'with', task.name)\n",
    "\n",
    "# start a tensorboard writer, writes to runs/\n",
    "# writer_path = f'runs/mnist_{task.nn}_Lr{settings.lr}_E{settings.epoch}_Bs{settings.bs}'\n",
    "comment = f'_R{settings.epoch}-B{settings.bs}-Lr{settings.lr}_{task.name}_{task.nn}'\n",
    "writer = SummaryWriter(comment=comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e6cc00",
   "metadata": {},
   "source": [
    "#### Setup the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58811232",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task.name == 'mnist':\n",
    "    if task.nn == 'cnn_wy':\n",
    "        model = CNNMnistWy().to(settings.device)\n",
    "    elif task.nn == 'cnn_wy_bn':\n",
    "        model = CNNMnistWyBn().to(settings.device)\n",
    "    elif task.nn == 'cnn_wy_bndp':\n",
    "        model = CNNMnistWyBnDp().to(settings.device)   \n",
    "    elif task.nn == 'cnn':\n",
    "        model = CNNMnist().to(settings.device)\n",
    "    elif task.nn == '2nn_wy':\n",
    "        model = TwoNN().to(settings.device)\n",
    "    else:\n",
    "        model = MLP().to(settings.device)\n",
    "    loader_train, loader_test = data_mnist(path=settings.datapath,batch_size=settings.bs)\n",
    "elif task.name == 'cifar':\n",
    "    if task.nn == 'torch':\n",
    "        model = CNNCifarTorch().to(settings.device)\n",
    "    elif task.nn == 'cnn_wy':\n",
    "        model = CNNCifarWy().to(settings.device)\n",
    "    elif task.nn == 'tf':\n",
    "        model = CNNCifarTf().to(settings.device)\n",
    "    elif task.nn == 'tfdp':\n",
    "        model = CNNCifarTfDp().to(settings.device)\n",
    "    loader_train, loader_test = data_cifar(path=settings.datapath,batch_size=settings.bs)\n",
    "else:\n",
    "    print('Unrecognizable model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f821f9e",
   "metadata": {},
   "source": [
    "#### Setup the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fdb81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss().to(settings.device)\n",
    "if settings.nesterov:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=settings.lr, momentum=settings.momentum, nesterov=settings.nesterov)\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=settings.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df05705",
   "metadata": {},
   "source": [
    "#### Show confirmation messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc13d526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model training has setup...\n",
      "\n",
      "Dataset:\tcifar\n",
      "Loss function:\tCrossEntropyLoss()\n",
      "Optimizer:\tvanilla SGD\n",
      "learning rate:\t0.1\n",
      "Batch size:\t100\n",
      "Num of epochs:\t100\n",
      "Model to train:\n",
      " CNNCifarTfDp(\n",
      "  (conv_layer): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): Dropout2d(p=0.5, inplace=False)\n",
      "    (8): ReLU()\n",
      "  )\n",
      "  (fc_layer): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Trainable model parameters:\t122570\n"
     ]
    }
   ],
   "source": [
    "# print some welcome messages\n",
    "print('\\nModel training has setup...\\n')\n",
    "print(f'Dataset:\\t{task.name}')\n",
    "print(f'Loss function:\\t{loss_fn}')\n",
    "print('Optimizer:\\tSGD with Nesterov momentum=0.9') if settings.nesterov else print('Optimizer:\\tvanilla SGD')\n",
    "print(f'learning rate:\\t{settings.lr}')\n",
    "print(f'Batch size:\\t{settings.bs}')\n",
    "print(f'Num of epochs:\\t{settings.epoch}')\n",
    "print('Model to train:\\n', model)\n",
    "print(f'Trainable model parameters:\\t{get_count_params(model)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a86e9",
   "metadata": {},
   "source": [
    "#### Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b532f95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 1 | Training Loss: 1.98 | Test Loss: 1.65 | Test accuracy = 39.53%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 2 | Training Loss: 1.60 | Test Loss: 1.45 | Test accuracy = 47.98%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 3 | Training Loss: 1.44 | Test Loss: 1.28 | Test accuracy = 53.78%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 4 | Training Loss: 1.33 | Test Loss: 1.17 | Test accuracy = 58.19%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 5 | Training Loss: 1.24 | Test Loss: 1.10 | Test accuracy = 60.92%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 6 | Training Loss: 1.16 | Test Loss: 1.03 | Test accuracy = 63.14%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 7 | Training Loss: 1.10 | Test Loss: 1.01 | Test accuracy = 64.05%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 8 | Training Loss: 1.05 | Test Loss: 0.95 | Test accuracy = 66.58%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 9 | Training Loss: 1.01 | Test Loss: 0.90 | Test accuracy = 68.52%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 10 | Training Loss: 0.97 | Test Loss: 0.92 | Test accuracy = 67.85%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 11 | Training Loss: 0.93 | Test Loss: 0.96 | Test accuracy = 65.97%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 12 | Training Loss: 0.90 | Test Loss: 0.89 | Test accuracy = 68.86%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 13 | Training Loss: 0.88 | Test Loss: 0.84 | Test accuracy = 71.03%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 14 | Training Loss: 0.86 | Test Loss: 0.82 | Test accuracy = 71.25%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 15 | Training Loss: 0.84 | Test Loss: 0.83 | Test accuracy = 71.44%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 16 | Training Loss: 0.82 | Test Loss: 0.79 | Test accuracy = 72.40%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 17 | Training Loss: 0.80 | Test Loss: 0.83 | Test accuracy = 71.27%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 18 | Training Loss: 0.78 | Test Loss: 0.79 | Test accuracy = 72.40%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 19 | Training Loss: 0.77 | Test Loss: 0.81 | Test accuracy = 72.09%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 20 | Training Loss: 0.76 | Test Loss: 0.79 | Test accuracy = 72.51%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 21 | Training Loss: 0.74 | Test Loss: 0.78 | Test accuracy = 72.77%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 22 | Training Loss: 0.73 | Test Loss: 0.75 | Test accuracy = 73.95%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 23 | Training Loss: 0.72 | Test Loss: 0.78 | Test accuracy = 72.87%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 24 | Training Loss: 0.71 | Test Loss: 0.79 | Test accuracy = 72.22%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 25 | Training Loss: 0.70 | Test Loss: 0.75 | Test accuracy = 73.56%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 26 | Training Loss: 0.69 | Test Loss: 0.76 | Test accuracy = 74.08%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 27 | Training Loss: 0.68 | Test Loss: 0.75 | Test accuracy = 73.73%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 28 | Training Loss: 0.67 | Test Loss: 0.78 | Test accuracy = 72.63%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 29 | Training Loss: 0.66 | Test Loss: 0.74 | Test accuracy = 74.14%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 30 | Training Loss: 0.65 | Test Loss: 0.75 | Test accuracy = 74.36%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 31 | Training Loss: 0.65 | Test Loss: 0.75 | Test accuracy = 74.24%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 32 | Training Loss: 0.64 | Test Loss: 0.75 | Test accuracy = 73.66%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 33 | Training Loss: 0.63 | Test Loss: 0.73 | Test accuracy = 74.57%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 34 | Training Loss: 0.62 | Test Loss: 0.75 | Test accuracy = 74.21%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 35 | Training Loss: 0.62 | Test Loss: 0.74 | Test accuracy = 74.14%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 36 | Training Loss: 0.62 | Test Loss: 0.75 | Test accuracy = 74.35%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 37 | Training Loss: 0.60 | Test Loss: 0.76 | Test accuracy = 74.31%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 38 | Training Loss: 0.60 | Test Loss: 0.75 | Test accuracy = 74.49%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 39 | Training Loss: 0.60 | Test Loss: 0.74 | Test accuracy = 74.61%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 40 | Training Loss: 0.59 | Test Loss: 0.79 | Test accuracy = 73.25%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 41 | Training Loss: 0.58 | Test Loss: 0.75 | Test accuracy = 74.64%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 42 | Training Loss: 0.58 | Test Loss: 0.79 | Test accuracy = 73.68%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 43 | Training Loss: 0.57 | Test Loss: 0.75 | Test accuracy = 74.69%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 44 | Training Loss: 0.57 | Test Loss: 0.74 | Test accuracy = 74.92%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 45 | Training Loss: 0.57 | Test Loss: 0.79 | Test accuracy = 73.20%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 46 | Training Loss: 0.56 | Test Loss: 0.76 | Test accuracy = 74.23%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 47 | Training Loss: 0.56 | Test Loss: 0.76 | Test accuracy = 74.77%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 48 | Training Loss: 0.56 | Test Loss: 0.76 | Test accuracy = 74.88%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 49 | Training Loss: 0.55 | Test Loss: 0.78 | Test accuracy = 73.99%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 50 | Training Loss: 0.55 | Test Loss: 0.76 | Test accuracy = 74.37%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 51 | Training Loss: 0.54 | Test Loss: 0.78 | Test accuracy = 74.16%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 52 | Training Loss: 0.54 | Test Loss: 0.76 | Test accuracy = 74.83%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 53 | Training Loss: 0.54 | Test Loss: 0.77 | Test accuracy = 74.82%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 54 | Training Loss: 0.54 | Test Loss: 0.77 | Test accuracy = 74.22%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 55 | Training Loss: 0.52 | Test Loss: 0.77 | Test accuracy = 74.59%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 56 | Training Loss: 0.53 | Test Loss: 0.78 | Test accuracy = 74.36%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 57 | Training Loss: 0.52 | Test Loss: 0.75 | Test accuracy = 74.96%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 58 | Training Loss: 0.52 | Test Loss: 0.75 | Test accuracy = 74.78%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 59 | Training Loss: 0.52 | Test Loss: 0.79 | Test accuracy = 73.66%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 60 | Training Loss: 0.52 | Test Loss: 0.79 | Test accuracy = 74.27%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 61 | Training Loss: 0.51 | Test Loss: 0.78 | Test accuracy = 74.89%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 62 | Training Loss: 0.50 | Test Loss: 0.77 | Test accuracy = 74.70%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 63 | Training Loss: 0.51 | Test Loss: 0.78 | Test accuracy = 74.68%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 64 | Training Loss: 0.50 | Test Loss: 0.78 | Test accuracy = 74.46%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 65 | Training Loss: 0.51 | Test Loss: 0.78 | Test accuracy = 73.81%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 66 | Training Loss: 0.50 | Test Loss: 0.78 | Test accuracy = 74.50%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 67 | Training Loss: 0.49 | Test Loss: 0.78 | Test accuracy = 73.99%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 68 | Training Loss: 0.49 | Test Loss: 0.80 | Test accuracy = 73.63%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 69 | Training Loss: 0.49 | Test Loss: 0.80 | Test accuracy = 73.79%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 70 | Training Loss: 0.49 | Test Loss: 0.79 | Test accuracy = 73.90%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 71 | Training Loss: 0.49 | Test Loss: 0.78 | Test accuracy = 74.70%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 72 | Training Loss: 0.49 | Test Loss: 0.78 | Test accuracy = 74.59%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 73 | Training Loss: 0.49 | Test Loss: 0.80 | Test accuracy = 74.70%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 74 | Training Loss: 0.48 | Test Loss: 0.78 | Test accuracy = 74.61%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 75 | Training Loss: 0.48 | Test Loss: 0.79 | Test accuracy = 74.83%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 76 | Training Loss: 0.48 | Test Loss: 0.82 | Test accuracy = 73.93%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 77 | Training Loss: 0.47 | Test Loss: 0.78 | Test accuracy = 75.13%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 78 | Training Loss: 0.48 | Test Loss: 0.80 | Test accuracy = 74.43%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 79 | Training Loss: 0.47 | Test Loss: 0.79 | Test accuracy = 74.49%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 80 | Training Loss: 0.47 | Test Loss: 0.79 | Test accuracy = 74.70%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 81 | Training Loss: 0.47 | Test Loss: 0.84 | Test accuracy = 73.68%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 82 | Training Loss: 0.46 | Test Loss: 0.79 | Test accuracy = 74.84%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 83 | Training Loss: 0.46 | Test Loss: 0.78 | Test accuracy = 75.18%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 84 | Training Loss: 0.46 | Test Loss: 0.81 | Test accuracy = 74.41%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 85 | Training Loss: 0.46 | Test Loss: 0.79 | Test accuracy = 75.04%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 86 | Training Loss: 0.45 | Test Loss: 0.85 | Test accuracy = 73.81%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 87 | Training Loss: 0.46 | Test Loss: 0.82 | Test accuracy = 74.62%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 88 | Training Loss: 0.46 | Test Loss: 0.79 | Test accuracy = 74.52%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 89 | Training Loss: 0.45 | Test Loss: 0.80 | Test accuracy = 74.23%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 90 | Training Loss: 0.45 | Test Loss: 0.81 | Test accuracy = 74.80%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 91 | Training Loss: 0.46 | Test Loss: 0.80 | Test accuracy = 74.11%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 92 | Training Loss: 0.45 | Test Loss: 0.81 | Test accuracy = 74.59%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 93 | Training Loss: 0.46 | Test Loss: 0.86 | Test accuracy = 73.06%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 94 | Training Loss: 0.45 | Test Loss: 0.81 | Test accuracy = 74.08%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 95 | Training Loss: 0.45 | Test Loss: 0.82 | Test accuracy = 74.46%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 96 | Training Loss: 0.45 | Test Loss: 0.82 | Test accuracy = 73.97%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 97 | Training Loss: 0.44 | Test Loss: 0.80 | Test accuracy = 74.62%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 98 | Training Loss: 0.44 | Test Loss: 0.85 | Test accuracy = 74.14%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 99 | Training Loss: 0.44 | Test Loss: 0.83 | Test accuracy = 73.85%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch: 100 | Training Loss: 0.44 | Test Loss: 0.81 | Test accuracy = 74.34%\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "\n",
      "Training completed, time elapsed on this device: 5077.21s\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "start = time.time()\n",
    "\n",
    "# start the training\n",
    "train_model(loader_train=loader_train,\n",
    "            loader_test=loader_test,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            decay=1.0,\n",
    "            epochs=settings.epoch,\n",
    "            device=settings.device)\n",
    "    \n",
    "# print the wall-clock-time used\n",
    "end=time.time() \n",
    "print('\\nTraining completed, time elapsed on this device: {:.2f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88e6acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flush and close the tensorboard writer\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e8355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
