{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afed2341",
   "metadata": {},
   "source": [
    "# About\n",
    "This is the step-by-step version of `./src/mybaseline-all-in-one.py`\n",
    "\n",
    "### Models\n",
    "Model `CNNMnistWyBn` adds batchnormalization `nn.BatchNorm2d` after convolutional filter `nn.Conv2d`, which leads to better performance in baseline learning of MNIST. However, in a test run under non-IID case {E=5, B=10, C=0.0} the validation performance is lower than original model `CNNMnistWy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704db042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfc931",
   "metadata": {},
   "source": [
    "##### Use tensorboard writer to visualize the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562118d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba339dbd",
   "metadata": {},
   "source": [
    "#### Option classes and help function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abdd105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskMnist():\n",
    "    def __init__(self, nn='cnn'):\n",
    "        self.path = '..\\data\\mnist'\n",
    "        self.name = 'mnist'\n",
    "        self.nn = nn\n",
    "        \n",
    "class TaskCifar():\n",
    "    def __init__(self,nn='torch'):\n",
    "        self.path = '..\\data\\cifar'\n",
    "        self.name = 'cifar10'\n",
    "        self.nn = nn\n",
    "\n",
    "class HyperParam():\n",
    "    def __init__(self,path,learning_rate=0.1, batch_size=100, epoch=10, momentum=0.9, nesterov=False):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.datapath = path\n",
    "        self.lr=learning_rate\n",
    "        self.bs=batch_size\n",
    "        self.epoch=epoch\n",
    "        self.momentum=momentum\n",
    "        self.nesterov=nesterov        \n",
    "    \n",
    "# the function used to count the number of trainable parameters\n",
    "def get_count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ec9ca",
   "metadata": {},
   "source": [
    "#### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb89ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 2NN model described in the vanilla FL paper for experiments with MNIST\n",
    "class TwoNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoNN,self).__init__()\n",
    "        self.nn_layer=nn.Sequential(\n",
    "            nn.Linear(in_features=28*28,out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100,out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100,out_features=10)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,28*28)\n",
    "        logits = self.nn_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "                 \n",
    "# the 2NN model in AshwinRJ's repository\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_input = nn.Linear(28*28, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.layer_hidden = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1]*x.shape[-2]*x.shape[-1])\n",
    "        x = self.layer_input(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_hidden(x)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "\n",
    "# the CNN model describted in the vanilla FL paper for experiments with MNIST\n",
    "class CNNMnistWy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnistWy,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512,out_features=10),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1,1024)\n",
    "        logits = self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "\n",
    "# Same with CNNMnistWy with batchnormalization\n",
    "class CNNMnistWyBn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnistWyBn,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512,out_features=10),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1,1024)\n",
    "        logits = self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "    \n",
    "# Same with CNNMnistWy with additional dropout layer after conv filter\n",
    "class CNNMnistWy2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnistWy2,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512,out_features=10),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1,1024)\n",
    "        logits = self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "\n",
    "# the CNN model in AshwinRJ's repository\n",
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# the example model used in the official CNN training tutorial of PyTorch using CIFAR10\n",
    "# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "class CNNCifarTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNCifarTorch,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(16*5*5,120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120,84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84,10)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1, 16 * 5 * 5)\n",
    "        logits=self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "\n",
    "# the exmaple model used in the official CNN tutorial of TensorFlow using CIFAR10\n",
    "# https://www.tensorflow.org/tutorials/images/cnn\n",
    "class CNNCifarTf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNCifarTf,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=32, kernel_size=3), # output size 30*30, i.e., (32, 30 ,30)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # output size 15*15, i.e., (32, 15 ,15)\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3), # output size 13*13, i.e., (64, 13 ,13)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # output size 6*6, i.e., (64, 6, 6)\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3), # output size 4*4, i.e., (64, 4, 4)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64,out_features=10),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1,1024)\n",
    "        logits=self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b594dc",
   "metadata": {},
   "source": [
    "#### Define data loader functions\n",
    "**BE CAUTION** that, the transform applied to the test data loader should be **the same as the training data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4017cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cifar(path, batch_size=100):\n",
    "    \"\"\"\n",
    "    returns training data loader and test data loader\n",
    "    \"\"\"\n",
    "    # no brainer normalization used in the pytorch tutorial\n",
    "    mean_0 = (0.5, 0.5, 0.5)\n",
    "    std_0 = (0.5, 0.5, 0.5)\n",
    "\n",
    "    # alternative normilzation\n",
    "    mean_1 = (0.4914, 0.4822, 0.4465)\n",
    "    std_1 = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "    # configure tranform for training data\n",
    "    # standard transform used in the pytorch tutorial \n",
    "    transform_train_0 = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_0,std_0),\n",
    "    ])\n",
    "\n",
    "    # configure transform for test data\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_0,std_0),\n",
    "    ])\n",
    "    \n",
    "    '''\n",
    "    # Alternative transform\n",
    "    # enhanced transform, random crop and flip is optional\n",
    "    transform_train_1 = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_1,std_1),\n",
    "    ])\n",
    "\n",
    "    # alternative, only random crop is used\n",
    "    transform_train_2 = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_1,std_1),\n",
    "    ])\n",
    "\n",
    "    # configure transform for test data\n",
    "    transform_test_1 = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_1,std_1),\n",
    "    ])\n",
    "    '''\n",
    "    # setup the CIFAR10 training dataset\n",
    "    data_train = datasets.CIFAR10(root=path, train=True, download=False, transform=transform_train_0)\n",
    "    loader_train = data.DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # setup the CIFAR10 test dataset\n",
    "    data_test = datasets.CIFAR10(root=path, train=False, download=False, transform=transform_test_0)\n",
    "    loader_test = data.DataLoader(data_test, batch_size=100, shuffle=False)\n",
    "\n",
    "    return loader_train, loader_test\n",
    "\n",
    "def data_mnist(path,batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # setup the MNIST training dataset\n",
    "    data_train = datasets.MNIST(root=path, train=True, download=False, transform=transform)\n",
    "    loader_train = data.DataLoader(data_train, batch_size=batch_size, shuffle=True) \n",
    "    \n",
    "    # setup the MNIST training dataset\n",
    "    data_test = datasets.MNIST(root=path, train=False, download=False, transform=transform)\n",
    "    loader_test = data.DataLoader(data_test, batch_size=100, shuffle=False)\n",
    "    return loader_train,loader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f29d31",
   "metadata": {},
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a00bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(loader_train, loader_test, epochs, loss_fn, optimizer, device):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "        test_acc = 0.0\n",
    "\n",
    "        # training of each epoch\n",
    "        model.train()\n",
    "        for batch, (images, labels) in enumerate(loader_train):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        train_loss /= len(loader_train.dataset)\n",
    "\n",
    "        # test after each epoch\n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        with torch.no_grad():\n",
    "            for batch, (images, labels) in enumerate(loader_test):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs,labels)\n",
    "                test_loss += loss.item() * images.size(0)\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                num_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        test_loss /= len(loader_test.dataset)\n",
    "        test_acc = 100*num_correct/len(loader_test.dataset)\n",
    "        print('Epoch: {} | Training Loss: {:.2f} | Test Loss: {:.2f} | Test accuracy = {:.2f}%'.format(epoch, train_loss, test_loss, test_acc))\n",
    "\n",
    "        # user tensorboard writer\n",
    "        writer.add_scalar(\"Training loss\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Test/Loss\", test_loss, epoch)\n",
    "        writer.add_scalar(\"Test/Acc\", test_acc, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd372ec",
   "metadata": {},
   "source": [
    "#### Setup the training task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12b8578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cnn_wy2 with mnist\n"
     ]
    }
   ],
   "source": [
    "# configure the task\n",
    "task = TaskMnist(nn='cnn_wy2')\n",
    "\n",
    "# configure the training parameters\n",
    "settings = HyperParam(path=task.path, learning_rate=0.1, nesterov=False, batch_size=32, epoch=100)\n",
    "print('Train', task.nn, 'with', task.name)\n",
    "\n",
    "# start a tensorboard writer, writes to runs/\n",
    "writer_path = f'runs/mnist_{task.nn}_Lr{settings.lr}_E{settings.epoch}_Bs{settings.bs}'\n",
    "writer = SummaryWriter(writer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e7c126",
   "metadata": {},
   "source": [
    "#### Setup the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fbbf174",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task.name == 'mnist':\n",
    "    if task.nn == 'cnn_wy':\n",
    "        model = CNNMnistWy().to(settings.device)\n",
    "    elif task.nn == 'cnn_wy_bn':\n",
    "        model = CNNMnistWyBn().to(settings.device)\n",
    "    elif task.nn == 'cnn_wy2':\n",
    "        model = CNNMnistWy2().to(settings.device)   \n",
    "    elif task.nn == 'cnn':\n",
    "        model = CNNMnist().to(settings.device)\n",
    "    elif task.nn == '2nn_wy':\n",
    "        model = TwoNN().to(settings.device)\n",
    "    else:\n",
    "        model = MLP().to(settings.device)\n",
    "    loader_train, loader_test = data_mnist(path=settings.datapath,batch_size=settings.bs)\n",
    "elif task.name == 'cifar':\n",
    "    if task.cnn == 'torch':\n",
    "        model = CNNCifarTorch().to(settings.device)\n",
    "    else:\n",
    "        model = CNNCifarTf().to(settings.device)\n",
    "    loader_train, loader_test = data_cifar(path=settings.datapath,batch_size=settings.bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74254f76",
   "metadata": {},
   "source": [
    "#### Setup the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3946107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss().to(settings.device)\n",
    "if settings.nesterov:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=settings.lr, momentum=settings.momentum, nesterov=settings.nesterov)\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=settings.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd3afe",
   "metadata": {},
   "source": [
    "#### Show confirmation messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a9380b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model training has setup...\n",
      "\n",
      "Dataset:\tmnist\n",
      "Loss function:\tCrossEntropyLoss()\n",
      "Optimizer:\tvanilla SGD\n",
      "learning rate:\t0.1\n",
      "Batch size:\t32\n",
      "Num of epochs:\t100\n",
      "Model to train:\n",
      " CNNMnistWy2(\n",
      "  (conv_layer): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layer): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Trainable model parameters:\t582026\n"
     ]
    }
   ],
   "source": [
    "# print some welcome messages\n",
    "print('\\nModel training has setup...\\n')\n",
    "print(f'Dataset:\\t{task.name}')\n",
    "print(f'Loss function:\\t{loss_fn}')\n",
    "print('Optimizer:\\tSGD with Nesterov momentum=0.9') if settings.nesterov else print('Optimizer:\\tvanilla SGD')\n",
    "print(f'learning rate:\\t{settings.lr}')\n",
    "print(f'Batch size:\\t{settings.bs}')\n",
    "print(f'Num of epochs:\\t{settings.epoch}')\n",
    "print('Model to train:\\n', model)\n",
    "print(f'Trainable model parameters:\\t{get_count_params(model)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1fa827",
   "metadata": {},
   "source": [
    "#### Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ada3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.14 | Test Loss: 0.04 | Test accuracy = 98.90%\n",
      "Epoch: 2 | Training Loss: 0.05 | Test Loss: 0.04 | Test accuracy = 99.11%\n",
      "Epoch: 3 | Training Loss: 0.04 | Test Loss: 0.02 | Test accuracy = 99.37%\n",
      "Epoch: 4 | Training Loss: 0.03 | Test Loss: 0.03 | Test accuracy = 99.20%\n",
      "Epoch: 5 | Training Loss: 0.03 | Test Loss: 0.02 | Test accuracy = 99.25%\n",
      "Epoch: 6 | Training Loss: 0.02 | Test Loss: 0.02 | Test accuracy = 99.32%\n",
      "Epoch: 7 | Training Loss: 0.02 | Test Loss: 0.02 | Test accuracy = 99.30%\n",
      "Epoch: 8 | Training Loss: 0.02 | Test Loss: 0.02 | Test accuracy = 99.27%\n",
      "Epoch: 9 | Training Loss: 0.02 | Test Loss: 0.02 | Test accuracy = 99.38%\n",
      "Epoch: 10 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.31%\n",
      "Epoch: 11 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.41%\n",
      "Epoch: 12 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.26%\n",
      "Epoch: 13 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.45%\n",
      "Epoch: 14 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.39%\n",
      "Epoch: 15 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.34%\n",
      "Epoch: 16 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.35%\n",
      "Epoch: 17 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.21%\n",
      "Epoch: 18 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.38%\n",
      "Epoch: 19 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.43%\n",
      "Epoch: 20 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.48%\n",
      "Epoch: 21 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.36%\n",
      "Epoch: 22 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.29%\n",
      "Epoch: 23 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.37%\n",
      "Epoch: 24 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.30%\n",
      "Epoch: 25 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.35%\n",
      "Epoch: 26 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.33%\n",
      "Epoch: 27 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.22%\n",
      "Epoch: 28 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.31%\n",
      "Epoch: 29 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.34%\n",
      "Epoch: 30 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.35%\n",
      "Epoch: 31 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.33%\n",
      "Epoch: 32 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.40%\n",
      "Epoch: 33 | Training Loss: 0.01 | Test Loss: 0.03 | Test accuracy = 99.30%\n",
      "Epoch: 34 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.38%\n",
      "Epoch: 35 | Training Loss: 0.01 | Test Loss: 0.03 | Test accuracy = 99.22%\n",
      "Epoch: 36 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.33%\n",
      "Epoch: 37 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.33%\n",
      "Epoch: 38 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.40%\n",
      "Epoch: 39 | Training Loss: 0.01 | Test Loss: 0.03 | Test accuracy = 99.25%\n",
      "Epoch: 40 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.37%\n",
      "Epoch: 41 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.37%\n",
      "Epoch: 42 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.40%\n",
      "Epoch: 43 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.50%\n",
      "Epoch: 44 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.36%\n",
      "Epoch: 45 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.30%\n",
      "Epoch: 46 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.52%\n",
      "Epoch: 47 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.56%\n",
      "Epoch: 48 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.32%\n",
      "Epoch: 49 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.30%\n",
      "Epoch: 50 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.33%\n",
      "Epoch: 51 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.37%\n",
      "Epoch: 52 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.48%\n",
      "Epoch: 53 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.49%\n",
      "Epoch: 54 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.48%\n",
      "Epoch: 55 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.40%\n",
      "Epoch: 56 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.50%\n",
      "Epoch: 57 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.48%\n",
      "Epoch: 58 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.30%\n",
      "Epoch: 59 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.08%\n",
      "Epoch: 60 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.42%\n",
      "Epoch: 61 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.40%\n",
      "Epoch: 62 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.39%\n",
      "Epoch: 63 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.45%\n",
      "Epoch: 64 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.42%\n",
      "Epoch: 65 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.32%\n",
      "Epoch: 66 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.33%\n",
      "Epoch: 67 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.30%\n",
      "Epoch: 68 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.41%\n",
      "Epoch: 69 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.42%\n",
      "Epoch: 70 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.41%\n",
      "Epoch: 71 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.37%\n",
      "Epoch: 72 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.49%\n",
      "Epoch: 73 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.44%\n",
      "Epoch: 74 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.34%\n",
      "Epoch: 75 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.46%\n",
      "Epoch: 76 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.55%\n",
      "Epoch: 77 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.31%\n",
      "Epoch: 78 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.54%\n",
      "Epoch: 79 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.37%\n",
      "Epoch: 80 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.37%\n",
      "Epoch: 81 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.34%\n",
      "Epoch: 82 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.33%\n",
      "Epoch: 83 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.37%\n",
      "Epoch: 84 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.33%\n",
      "Epoch: 85 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.34%\n",
      "Epoch: 86 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.35%\n",
      "Epoch: 87 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.39%\n",
      "Epoch: 88 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.39%\n",
      "Epoch: 89 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.40%\n",
      "Epoch: 90 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.36%\n",
      "Epoch: 91 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.39%\n",
      "Epoch: 92 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.44%\n",
      "Epoch: 93 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.42%\n",
      "Epoch: 94 | Training Loss: 0.00 | Test Loss: 0.04 | Test accuracy = 99.32%\n",
      "Epoch: 95 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.37%\n",
      "Epoch: 96 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.41%\n",
      "Epoch: 97 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.38%\n",
      "Epoch: 98 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.52%\n",
      "Epoch: 99 | Training Loss: 0.00 | Test Loss: 0.03 | Test accuracy = 99.44%\n",
      "Epoch: 100 | Training Loss: 0.00 | Test Loss: 0.02 | Test accuracy = 99.43%\n",
      "\n",
      "Training completed, time elapsed on this device: 1226.87s\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "start = time.time()\n",
    "\n",
    "# start the training\n",
    "train_model(loader_train=loader_train,\n",
    "            loader_test=loader_test,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            epochs=settings.epoch,\n",
    "            device=settings.device)\n",
    "    \n",
    "# print the wall-clock-time used\n",
    "end=time.time() \n",
    "print('\\nTraining completed, time elapsed on this device: {:.2f}s'.format(end-start))\n",
    "\n",
    "# flush and close the tensorboard writer\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a3cf05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
