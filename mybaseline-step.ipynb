{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1e1709",
   "metadata": {},
   "source": [
    "# About\n",
    "This is the step-by-step version of `./src/mybaseline-all-in-one.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5010a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaaf3c0",
   "metadata": {},
   "source": [
    "#### Option classes and help function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b1bf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskMnist():\n",
    "    def __init__(self, nn='cnn'):\n",
    "        self.path = '.\\data\\mnist'\n",
    "        self.name = 'mnist'\n",
    "        self.nn = nn\n",
    "        \n",
    "class TaskCifar():\n",
    "    def __init__(self,nn='torch'):\n",
    "        self.path = '.\\data\\cifar'\n",
    "        self.name = 'cifar10'\n",
    "        self.nn = nn\n",
    "\n",
    "class HyperParam():\n",
    "    def __init__(self,path,learning_rate=0.1, batch_size=100, epoch=10, momentum=0.9, nesterov=False):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.datapath = path\n",
    "        self.lr=learning_rate\n",
    "        self.bs=batch_size\n",
    "        self.epoch=epoch\n",
    "        self.momentum=momentum\n",
    "        self.nesterov=nesterov        \n",
    "    \n",
    "# the function used to count the number of trainable parameters\n",
    "def get_count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c757c65",
   "metadata": {},
   "source": [
    "#### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0eb551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 2NN model described in the vanilla FL paper for experiments with MNIST\n",
    "class TwoNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoNN,self).__init__()\n",
    "        self.nn_layer=nn.Sequential(\n",
    "            nn.Linear(in_features=28*28,out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100,out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100,out_features=10)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,28*28)\n",
    "        logits = self.nn_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "                 \n",
    "# the 2NN model in AshwinRJ's repository\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_input = nn.Linear(28*28, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.layer_hidden = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1]*x.shape[-2]*x.shape[-1])\n",
    "        x = self.layer_input(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_hidden(x)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "\n",
    "# the CNN model describted in the vanilla FL paper for experiments with MNIST\n",
    "class CNNMnistWy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnistWy,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512,out_features=10),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1,1024)\n",
    "        logits = self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "\n",
    "# the CNN model in AshwinRJ's repository\n",
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# the example model used in the official CNN training tutorial of PyTorch using CIFAR10\n",
    "# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "class CNNCifarTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNCifarTorch,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(16*5*5,120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120,84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84,10)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1, 16 * 5 * 5)\n",
    "        logits=self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)\n",
    "\n",
    "# the exmaple model used in the official CNN tutorial of TensorFlow using CIFAR10\n",
    "# https://www.tensorflow.org/tutorials/images/cnn\n",
    "class CNNCifarTf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNCifarTf,self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=32, kernel_size=3), # output size 30*30, i.e., (32, 30 ,30)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # output size 15*15, i.e., (32, 15 ,15)\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3), # output size 13*13, i.e., (64, 13 ,13)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # output size 6*6, i.e., (64, 6, 6)\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3), # output size 4*4, i.e., (64, 4, 4)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64,out_features=10),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv_layer(x)\n",
    "        x=x.view(-1,1024)\n",
    "        logits=self.fc_layer(x)\n",
    "        return F.log_softmax(logits,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dff5da",
   "metadata": {},
   "source": [
    "#### Define data loader functions\n",
    "**BE CAUTION** that, the transform applied to the test data loader should be **the same as the training data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f396ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cifar(path, batch_size=100):\n",
    "    \"\"\"\n",
    "    returns training data loader and test data loader\n",
    "    \"\"\"\n",
    "    # no brainer normalization used in the pytorch tutorial\n",
    "    mean_0 = (0.5, 0.5, 0.5)\n",
    "    std_0 = (0.5, 0.5, 0.5)\n",
    "\n",
    "    # alternative normilzation\n",
    "    mean_1 = (0.4914, 0.4822, 0.4465)\n",
    "    std_1 = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "    # configure tranform for training data\n",
    "    # standard transform used in the pytorch tutorial \n",
    "    transform_train_0 = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_0,std_0),\n",
    "    ])\n",
    "\n",
    "    # configure transform for test data\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_0,std_0),\n",
    "    ])\n",
    "    \n",
    "    '''\n",
    "    # Alternative transform\n",
    "    # enhanced transform, random crop and flip is optional\n",
    "    transform_train_1 = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_1,std_1),\n",
    "    ])\n",
    "\n",
    "    # alternative, only random crop is used\n",
    "    transform_train_2 = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_1,std_1),\n",
    "    ])\n",
    "\n",
    "    # configure transform for test data\n",
    "    transform_test_1 = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_1,std_1),\n",
    "    ])\n",
    "    '''\n",
    "    # setup the CIFAR10 training dataset\n",
    "    data_train = datasets.CIFAR10(root=path, train=True, download=False, transform=transform_train_0)\n",
    "    loader_train = data.DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # setup the CIFAR10 test dataset\n",
    "    data_test = datasets.CIFAR10(root=path, train=False, download=False, transform=transform_test_0)\n",
    "    loader_test = data.DataLoader(data_test, batch_size=100, shuffle=False)\n",
    "\n",
    "    return loader_train, loader_test\n",
    "\n",
    "def data_mnist(path,batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # setup the MNIST training dataset\n",
    "    data_train = datasets.MNIST(root=path, train=True, download=False, transform=transform)\n",
    "    loader_train = data.DataLoader(data_train, batch_size=batch_size, shuffle=True) \n",
    "    \n",
    "    # setup the MNIST training dataset\n",
    "    data_test = datasets.MNIST(root=path, train=False, download=False, transform=transform)\n",
    "    loader_test = data.DataLoader(data_test, batch_size=100, shuffle=False)\n",
    "    return loader_train,loader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5affea7a",
   "metadata": {},
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9daed453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(loader_train, loader_test, epochs, loss_fn, optimizer, device):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "        test_acc = 0.0\n",
    "\n",
    "        # training of each epoch\n",
    "        model.train()\n",
    "        for batch, (images, labels) in enumerate(loader_train):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        train_loss /= len(loader_train.dataset)\n",
    "\n",
    "        # test after each epoch\n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        with torch.no_grad():\n",
    "            for batch, (images, labels) in enumerate(loader_test):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs,labels)\n",
    "                test_loss += loss.item() * images.size(0)\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                num_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        test_loss /= len(loader_test.dataset)\n",
    "        test_acc = 100*num_correct/len(loader_test.dataset)\n",
    "        print('Epoch: {} | Training Loss: {:.2f} | Test Loss: {:.2f} | Test accuracy = {:.2f}%'.format(epoch, train_loss, test_loss, test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3872056",
   "metadata": {},
   "source": [
    "#### Setup the training task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "888c9f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cnn_wy with mnist\n"
     ]
    }
   ],
   "source": [
    "# configure the task\n",
    "task = TaskMnist(nn='cnn_wy')\n",
    "\n",
    "# configure the training parameters\n",
    "settings = HyperParam(path=task.path, learning_rate=0.01, nesterov=True)\n",
    "print('Train', task.nn, 'with', task.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1e970",
   "metadata": {},
   "source": [
    "#### Setup the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42a38549",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task.name == 'mnist':\n",
    "    if task.nn == 'cnn_wy':\n",
    "        model = CNNMnistWy().to(settings.device)\n",
    "    elif task.nn == 'cnn':\n",
    "        model = CNNMnist().to(settings.device)\n",
    "    elif task.nn == '2nn_wy':\n",
    "        model = TwoNN().to(settings.device)\n",
    "    else:\n",
    "        model = MLP().to(settings.device)\n",
    "    loader_train, loader_test = data_mnist(path=settings.datapath,batch_size=settings.bs)\n",
    "elif task.name == 'cifar':\n",
    "    if task.cnn == 'torch':\n",
    "        model = CNNCifarTorch().to(settings.device)\n",
    "    else:\n",
    "        model = CNNCifarTf().to(settings.device)\n",
    "    loader_train, loader_test = data_cifar(path=settings.datapath,batch_size=settings.bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e582b",
   "metadata": {},
   "source": [
    "#### Setup the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e76d15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss().to(settings.device)\n",
    "if settings.nesterov:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=settings.lr, momentum=settings.momentum, nesterov=settings.nesterov)\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=settings.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa8c38",
   "metadata": {},
   "source": [
    "#### Show confirmation messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b40935ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model training has setup...\n",
      "\n",
      "Dataset:\tmnist\n",
      "Loss function:\tCrossEntropyLoss()\n",
      "Optimizer:\tSGD with Nesterov momentum=0.9\n",
      "learning rate:\t0.01\n",
      "Batch size:\t100\n",
      "Num of epochs:\t10\n",
      "Model to train:\n",
      " CNNMnistWy(\n",
      "  (conv_layer): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layer): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Trainable model parameters:\t582026\n"
     ]
    }
   ],
   "source": [
    "# print some welcome messages\n",
    "print('\\nModel training has setup...\\n')\n",
    "print(f'Dataset:\\t{task.name}')\n",
    "print(f'Loss function:\\t{loss_fn}')\n",
    "print('Optimizer:\\tSGD with Nesterov momentum=0.9') if settings.nesterov else print('Optimizer:\\tvanilla SGD')\n",
    "print(f'learning rate:\\t{settings.lr}')\n",
    "print(f'Batch size:\\t{settings.bs}')\n",
    "print(f'Num of epochs:\\t{settings.epoch}')\n",
    "print('Model to train:\\n', model)\n",
    "print(f'Trainable model parameters:\\t{get_count_params(model)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d70e0",
   "metadata": {},
   "source": [
    "#### Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcccf8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.21 | Test Loss: 0.05 | Test accuracy = 98.21%\n",
      "Epoch: 2 | Training Loss: 0.05 | Test Loss: 0.04 | Test accuracy = 98.69%\n",
      "Epoch: 3 | Training Loss: 0.03 | Test Loss: 0.04 | Test accuracy = 98.86%\n",
      "Epoch: 4 | Training Loss: 0.03 | Test Loss: 0.04 | Test accuracy = 98.73%\n",
      "Epoch: 5 | Training Loss: 0.02 | Test Loss: 0.03 | Test accuracy = 99.19%\n",
      "Epoch: 6 | Training Loss: 0.02 | Test Loss: 0.02 | Test accuracy = 99.23%\n",
      "Epoch: 7 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.12%\n",
      "Epoch: 8 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.20%\n",
      "Epoch: 9 | Training Loss: 0.01 | Test Loss: 0.02 | Test accuracy = 99.19%\n",
      "Epoch: 10 | Training Loss: 0.01 | Test Loss: 0.03 | Test accuracy = 99.25%\n",
      "\n",
      "Training completed, time elapsed on this device: 106.61s\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "start = time.time()\n",
    "\n",
    "# start the training\n",
    "train_model(loader_train=loader_train,\n",
    "            loader_test=loader_test,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            epochs=settings.epoch,\n",
    "            device=settings.device)\n",
    "    \n",
    "# print the wall-clock-time used\n",
    "end=time.time() \n",
    "print('\\nTraining completed, time elapsed on this device: {:.2f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d24d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
