{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2477fe3",
   "metadata": {},
   "source": [
    "# Manually update model parameters\n",
    "The following method is learned from official tutorial *Deep learning with pytorch: a 60 mins blits/neural networks*\n",
    "\n",
    "See more information [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#update-the-weights)\n",
    "\n",
    "After performing `loss.backward()` to compute gradients for all parameters, instead of using optimizer (algorithm) wrapped in package `torch.optim` and model update function `optimizer.step()`, one can do the following steps to update the parameters manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f72915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose the learning rate has been set\n",
    "learning_rate = 0.01\n",
    "\n",
    "# use this loop to update every parameters of the model\n",
    "for param in model.parameters():\n",
    "    param.data.sub_(param.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bda9f",
   "metadata": {},
   "source": [
    "##### Partial parameter update\n",
    "Alternatively, if only some of the parameter(s) need to be updated, e.g., in the linear regression model, one may choose to update the weight `model.linear.weight`, then the following can be implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear.weight.data = model.linear.weight.data - learning_rate * model.linear.weight.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c95d4",
   "metadata": {},
   "source": [
    "or equivalently,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear.weight.data.sub_(learning_rate * model.linear.weight.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c356f026",
   "metadata": {},
   "source": [
    "In the above lines, we assumed that our `model` is an instance of some linear model class which has only one linear layer crated using `torch.nn.Linear()`, so that it has two parameters `model.linear.weight` and `model.linear.bias`. If we create a simple model from using polynomial directly, say, an syntax represents $y=f(x;w,b)$, having `loss` defined properly and `loss.backward()` executed, then the manual model update can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cff317",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w -= learning_rate * w.grad\n",
    "    b -= learning_rate * b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85301871",
   "metadata": {},
   "source": [
    "Note that, in order to use `w.grad` and `b.grad`, one need to toggle `requires_grad=True` when create/initialize these two parameters.  Wrapping in `torch.no_grad()` because weights have requires_grad=True, but we don't need to track this in autograd."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
